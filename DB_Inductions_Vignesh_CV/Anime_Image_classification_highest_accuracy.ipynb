{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This Anime image classification was created by Vignesh SS for Databyte Inductions. Adding further training data, more layers of CNN, with more iterations will even produce a higher accuracy i.e. greater than 73.64% received by this model, but at the cost of higher computation."
      ],
      "metadata": {
        "id": "tNep3dmNV0Vc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDVdCrPjoPuP",
        "outputId": "2776941f-eb39-43b1-f876-b0fa430967cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bing-image-downloader in /usr/local/lib/python3.10/dist-packages (1.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install bing-image-downloader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir Input_images\n",
        "\n",
        "from bing_image_downloader import downloader"
      ],
      "metadata": {
        "id": "N6A0_BRWo3HA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "characters = [\n",
        "    \"Iron Man\", \"Spider-Man\", \"Captain America\", \"Hulk\", \"Thor\",\n",
        "    \"Black Widow\", \"Doctor Strange\", \"Black Panther\", \"Captain Marvel\", \"Ant-Man\",\n",
        "    \"Naruto\", \"Goku\", \"Luffy\", \"Saitama\", \"Deku\",\n",
        "    \"Sasuke\", \"Vegeta\", \"Ichigo\", \"Natsu\", \"Eren\"\n",
        "]"
      ],
      "metadata": {
        "id": "XZe5okMjqlC-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to download images for each character\n",
        "def download_images(character_list, limit=60):     #Using 60 images instead of 30\n",
        "    for character in character_list:\n",
        "        print(f\"Downloading images for {character}...\")\n",
        "        downloader.download(character + \" figurine\", limit=limit, output_dir='Input_images/Figurines', adult_filter_off=True)\n",
        "        print(f\"Downloaded images for {character}\")"
      ],
      "metadata": {
        "id": "A1342jVosyvh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download images for all characters\n",
        "download_images(characters)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mg6ndinFtTPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset is downloaded now for 20 characters with 60 images for each character making it to function well."
      ],
      "metadata": {
        "id": "v_78S5YouHQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n"
      ],
      "metadata": {
        "id": "-uCkmhWZuYHT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the dataset\n",
        "data_dir = 'Input_images/Figurines'"
      ],
      "metadata": {
        "id": "E1eG4rfaujnd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ImageDataGenerator for data augmentation and preprocessing\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,              # Normalize pixel values to [0,1]\n",
        "    rotation_range=10,           # Rotate images by up to 10 degrees\n",
        "    width_shift_range=0.1,       # Shift the width of images by up to 10%\n",
        "    height_shift_range=0.1,      # Shift the height of images by up to 10%\n",
        "    shear_range=0.2,             # Randomly apply shearing\n",
        "    zoom_range=0.2,              # Randomly zoom into images\n",
        "    horizontal_flip=True,        # Randomly flip images horizontally\n",
        "    brightness_range=[0.8, 1.2], # Adjust brightness\n",
        "    #contrast_range=[0.8, 1.2],   # Adjust contrast\n",
        "    validation_split=0.2         # Split for validation\n",
        ")\n"
      ],
      "metadata": {
        "id": "y8BT-m9362yk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training data generator\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(150, 150),  # Resize images to 150x150\n",
        "    batch_size=32,           # Number of images to process in a batch\n",
        "    class_mode='categorical',# Use categorical labels\n",
        "    subset='training',        # Specify this is the training subset\n",
        "    shuffle=True            # Shuffle the training data\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjzcyZj066ll",
        "outputId": "71c30412-74de-4d85-f970-3ebad38cfdee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 960 images belonging to 20 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create validation data generator\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(150, 150),  # Resize images to 150x150\n",
        "    batch_size=32,           # Number of images to process in a batch\n",
        "    class_mode='categorical',# Use categorical labels\n",
        "    subset='validation',     # Specify this is the validation subset\n",
        "    shuffle=True            # Shuffle the validation data ( Additionally added)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGoSGe2t7AfP",
        "outputId": "54c84df6-bf5c-47d1-d48d-5e4d30e626e9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 239 images belonging to 20 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets create our CNN model"
      ],
      "metadata": {
        "id": "i-W-Ku8X-a58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
      ],
      "metadata": {
        "id": "ngbvCenn7JYX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(len(train_generator.class_indices), activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "y-QamkFh-ZtZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "YAzJdK9w-plW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_accuracy', mode='max'),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, mode='max')\n",
        "]"
      ],
      "metadata": {
        "id": "_lQaIXJ_pXp8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
        "    epochs=100,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIiPE1tl-r88",
        "outputId": "389e7f95-6609-486e-f20a-15f6dfdd1e84"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "30/30 [==============================] - 41s 1s/step - loss: 7.3205 - accuracy: 0.1375 - val_loss: 5.3529 - val_accuracy: 0.0491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100\n",
            "30/30 [==============================] - 30s 1s/step - loss: 3.4736 - accuracy: 0.1729 - val_loss: 11.2332 - val_accuracy: 0.0580\n",
            "Epoch 3/100\n",
            "30/30 [==============================] - 28s 954ms/step - loss: 2.8712 - accuracy: 0.2250 - val_loss: 11.6885 - val_accuracy: 0.0446\n",
            "Epoch 4/100\n",
            "30/30 [==============================] - 29s 975ms/step - loss: 2.5808 - accuracy: 0.2479 - val_loss: 17.7373 - val_accuracy: 0.0625\n",
            "Epoch 5/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 2.4248 - accuracy: 0.2948 - val_loss: 15.2973 - val_accuracy: 0.0491\n",
            "Epoch 6/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 2.3081 - accuracy: 0.3187 - val_loss: 19.7741 - val_accuracy: 0.0670\n",
            "Epoch 7/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 2.2192 - accuracy: 0.3708 - val_loss: 18.8077 - val_accuracy: 0.0982\n",
            "Epoch 8/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 2.1975 - accuracy: 0.3531 - val_loss: 13.2889 - val_accuracy: 0.0804\n",
            "Epoch 9/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 1.9577 - accuracy: 0.4229 - val_loss: 14.9411 - val_accuracy: 0.1071\n",
            "Epoch 10/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 1.9988 - accuracy: 0.4365 - val_loss: 10.8344 - val_accuracy: 0.1161\n",
            "Epoch 11/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 1.8491 - accuracy: 0.4698 - val_loss: 11.7267 - val_accuracy: 0.1384\n",
            "Epoch 12/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 1.8960 - accuracy: 0.4323 - val_loss: 6.1489 - val_accuracy: 0.2143\n",
            "Epoch 13/100\n",
            "30/30 [==============================] - 29s 966ms/step - loss: 1.6969 - accuracy: 0.4760 - val_loss: 6.7145 - val_accuracy: 0.2321\n",
            "Epoch 14/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 1.6979 - accuracy: 0.5073 - val_loss: 5.0184 - val_accuracy: 0.2232\n",
            "Epoch 15/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 1.6089 - accuracy: 0.5323 - val_loss: 3.1100 - val_accuracy: 0.3616\n",
            "Epoch 16/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 1.5978 - accuracy: 0.5302 - val_loss: 4.9583 - val_accuracy: 0.3125\n",
            "Epoch 17/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 1.4900 - accuracy: 0.5448 - val_loss: 4.7861 - val_accuracy: 0.3839\n",
            "Epoch 18/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 1.6451 - accuracy: 0.5344 - val_loss: 2.6624 - val_accuracy: 0.4196\n",
            "Epoch 19/100\n",
            "30/30 [==============================] - 29s 972ms/step - loss: 1.4952 - accuracy: 0.5562 - val_loss: 4.0150 - val_accuracy: 0.3973\n",
            "Epoch 20/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 1.4294 - accuracy: 0.5771 - val_loss: 3.5664 - val_accuracy: 0.3884\n",
            "Epoch 21/100\n",
            "30/30 [==============================] - 28s 952ms/step - loss: 1.4451 - accuracy: 0.5573 - val_loss: 2.2955 - val_accuracy: 0.4866\n",
            "Epoch 22/100\n",
            "30/30 [==============================] - 29s 984ms/step - loss: 1.3091 - accuracy: 0.6031 - val_loss: 2.6288 - val_accuracy: 0.4330\n",
            "Epoch 23/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 1.2688 - accuracy: 0.6115 - val_loss: 2.1630 - val_accuracy: 0.4107\n",
            "Epoch 24/100\n",
            "30/30 [==============================] - 29s 961ms/step - loss: 1.1887 - accuracy: 0.6562 - val_loss: 1.7487 - val_accuracy: 0.5357\n",
            "Epoch 25/100\n",
            "30/30 [==============================] - 29s 982ms/step - loss: 1.1640 - accuracy: 0.6417 - val_loss: 2.3642 - val_accuracy: 0.5000\n",
            "Epoch 26/100\n",
            "30/30 [==============================] - 29s 968ms/step - loss: 1.1667 - accuracy: 0.6438 - val_loss: 2.0364 - val_accuracy: 0.5580\n",
            "Epoch 27/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 1.0348 - accuracy: 0.6792 - val_loss: 1.7074 - val_accuracy: 0.5759\n",
            "Epoch 28/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 1.0947 - accuracy: 0.6781 - val_loss: 1.8696 - val_accuracy: 0.5580\n",
            "Epoch 29/100\n",
            "30/30 [==============================] - 28s 954ms/step - loss: 1.0704 - accuracy: 0.6958 - val_loss: 1.9162 - val_accuracy: 0.5670\n",
            "Epoch 30/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 1.0600 - accuracy: 0.6948 - val_loss: 2.0349 - val_accuracy: 0.4955\n",
            "Epoch 31/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 0.9992 - accuracy: 0.6875 - val_loss: 1.8081 - val_accuracy: 0.5045\n",
            "Epoch 32/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 0.9409 - accuracy: 0.7021 - val_loss: 1.8080 - val_accuracy: 0.5893\n",
            "Epoch 33/100\n",
            "30/30 [==============================] - 28s 949ms/step - loss: 0.8803 - accuracy: 0.7302 - val_loss: 1.6310 - val_accuracy: 0.6295\n",
            "Epoch 34/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 0.8685 - accuracy: 0.7542 - val_loss: 1.8248 - val_accuracy: 0.5848\n",
            "Epoch 35/100\n",
            "30/30 [==============================] - 28s 935ms/step - loss: 0.8056 - accuracy: 0.7552 - val_loss: 1.4836 - val_accuracy: 0.5982\n",
            "Epoch 36/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 0.7771 - accuracy: 0.7490 - val_loss: 1.5447 - val_accuracy: 0.6741\n",
            "Epoch 37/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 0.7124 - accuracy: 0.7833 - val_loss: 1.7027 - val_accuracy: 0.6071\n",
            "Epoch 38/100\n",
            "30/30 [==============================] - 28s 929ms/step - loss: 0.7976 - accuracy: 0.7531 - val_loss: 1.8679 - val_accuracy: 0.6027\n",
            "Epoch 39/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 0.8052 - accuracy: 0.7573 - val_loss: 1.8514 - val_accuracy: 0.5848\n",
            "Epoch 40/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 0.7280 - accuracy: 0.7760 - val_loss: 1.5183 - val_accuracy: 0.6830\n",
            "Epoch 41/100\n",
            "30/30 [==============================] - 28s 955ms/step - loss: 0.7808 - accuracy: 0.7625 - val_loss: 1.3903 - val_accuracy: 0.6473\n",
            "Epoch 42/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 0.7957 - accuracy: 0.7698 - val_loss: 1.5286 - val_accuracy: 0.6473\n",
            "Epoch 43/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 0.7352 - accuracy: 0.7792 - val_loss: 1.6851 - val_accuracy: 0.6429\n",
            "Epoch 44/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 0.6324 - accuracy: 0.7990 - val_loss: 2.1063 - val_accuracy: 0.5446\n",
            "Epoch 45/100\n",
            "30/30 [==============================] - 28s 957ms/step - loss: 0.6486 - accuracy: 0.7948 - val_loss: 2.3001 - val_accuracy: 0.5268\n",
            "Epoch 46/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 0.6675 - accuracy: 0.8073 - val_loss: 2.6914 - val_accuracy: 0.4509\n",
            "Epoch 47/100\n",
            "30/30 [==============================] - 28s 956ms/step - loss: 0.6612 - accuracy: 0.7927 - val_loss: 1.6530 - val_accuracy: 0.6116\n",
            "Epoch 48/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 0.6199 - accuracy: 0.8198 - val_loss: 1.6174 - val_accuracy: 0.6786\n",
            "Epoch 49/100\n",
            "30/30 [==============================] - 28s 945ms/step - loss: 0.6106 - accuracy: 0.8177 - val_loss: 1.2581 - val_accuracy: 0.6964\n",
            "Epoch 50/100\n",
            "30/30 [==============================] - 33s 1s/step - loss: 0.6407 - accuracy: 0.8188 - val_loss: 1.4294 - val_accuracy: 0.7054\n",
            "Epoch 51/100\n",
            "30/30 [==============================] - 29s 955ms/step - loss: 0.5012 - accuracy: 0.8396 - val_loss: 1.4609 - val_accuracy: 0.6786\n",
            "Epoch 52/100\n",
            "30/30 [==============================] - 28s 937ms/step - loss: 0.3883 - accuracy: 0.8823 - val_loss: 1.2335 - val_accuracy: 0.7411\n",
            "Epoch 53/100\n",
            "30/30 [==============================] - 28s 934ms/step - loss: 0.5669 - accuracy: 0.8354 - val_loss: 1.7856 - val_accuracy: 0.6607\n",
            "Epoch 54/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 0.5369 - accuracy: 0.8417 - val_loss: 1.2949 - val_accuracy: 0.6607\n",
            "Epoch 55/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 0.5945 - accuracy: 0.8323 - val_loss: 1.6034 - val_accuracy: 0.6518\n",
            "Epoch 56/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 0.5021 - accuracy: 0.8438 - val_loss: 1.2475 - val_accuracy: 0.7232\n",
            "Epoch 57/100\n",
            "30/30 [==============================] - 28s 933ms/step - loss: 0.4909 - accuracy: 0.8562 - val_loss: 1.7187 - val_accuracy: 0.6920\n",
            "Epoch 58/100\n",
            "30/30 [==============================] - 29s 981ms/step - loss: 0.5006 - accuracy: 0.8448 - val_loss: 1.4258 - val_accuracy: 0.6875\n",
            "Epoch 59/100\n",
            "30/30 [==============================] - 31s 1s/step - loss: 0.4666 - accuracy: 0.8646 - val_loss: 2.2832 - val_accuracy: 0.6384\n",
            "Epoch 60/100\n",
            "30/30 [==============================] - 32s 1s/step - loss: 0.5259 - accuracy: 0.8510 - val_loss: 1.5903 - val_accuracy: 0.6786\n",
            "Epoch 61/100\n",
            "30/30 [==============================] - 28s 945ms/step - loss: 0.5873 - accuracy: 0.8354 - val_loss: 1.9590 - val_accuracy: 0.6161\n",
            "Epoch 62/100\n",
            "30/30 [==============================] - 28s 947ms/step - loss: 0.4705 - accuracy: 0.8646 - val_loss: 1.2519 - val_accuracy: 0.7232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "model = tf.keras.models.load_model('best_model.h5')"
      ],
      "metadata": {
        "id": "z08_KbcppuBP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(validation_generator)\n",
        "print(f'Validation Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_7trfBr-scO",
        "outputId": "7e374a44-e6f5-4c27-8e24-df771592b0f2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 8s 909ms/step - loss: 1.4218 - accuracy: 0.7364\n",
            "Validation Accuracy: 73.64%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict the character from a new image\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "def predict_image(image_path):\n",
        "    img = image.load_img(image_path, target_size=(150, 150))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array /= 255.0\n",
        "\n",
        "    prediction = model.predict(img_array)\n",
        "    class_idx = np.argmax(prediction)\n",
        "    class_labels = list(train_generator.class_indices.keys())\n",
        "    return class_labels[class_idx]\n",
        "\n"
      ],
      "metadata": {
        "id": "Ki6vFEIy_R08"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20lhRVYJFsBU",
        "outputId": "d7c72cc0-0b1c-433b-cb83-35ab838420d3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=60aaae7d8a9c042a5b53a434195c7af4e73e79b3d41011db910f8e84846b5a78\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "image_url = \"https://ichef.bbci.co.uk/images/ic/1200x675/p09t1hg0.jpg\"\n",
        "image_filename = \"my_favourite.jpg\"\n",
        "wget.download(image_url, image_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "96LgQpA-Fv25",
        "outputId": "6489ade4-1d00-4867-dd71-af3ce8b0d856"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'my_favourite.jpg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Test the prediction function\n",
        "image_path = \"my_favourite.jpg\"\n",
        "print(f'The given image is: {predict_image(image_path)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETDJ2bsM_ih5",
        "outputId": "695a06ea-a672-4048-ec65-fca8727c3547"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 576ms/step\n",
            "The given image is: Captain America figurine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Print the class indices to verify correct labeling\n",
        "print(\"Class indices:\", train_generator.class_indices)\n",
        "\n",
        "# Evaluate the model and print the accuracy\n",
        "loss, accuracy = model.evaluate(validation_generator)\n",
        "print(f'Validation Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Check the class indices\n",
        "class_labels = list(train_generator.class_indices.keys())\n",
        "print(f'Class labels: {class_labels}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCJKF9TmUEX7",
        "outputId": "f9a116de-0c6c-4faf-c788-d99ffee2e13a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class indices: {'Ant-Man figurine': 0, 'Black Panther figurine': 1, 'Black Widow figurine': 2, 'Captain America figurine': 3, 'Captain Marvel figurine': 4, 'Deku figurine': 5, 'Doctor Strange figurine': 6, 'Eren figurine': 7, 'Goku figurine': 8, 'Hulk figurine': 9, 'Ichigo figurine': 10, 'Iron Man figurine': 11, 'Luffy figurine': 12, 'Naruto figurine': 13, 'Natsu figurine': 14, 'Saitama figurine': 15, 'Sasuke figurine': 16, 'Spider-Man figurine': 17, 'Thor figurine': 18, 'Vegeta figurine': 19}\n",
            "8/8 [==============================] - 8s 1s/step - loss: 1.2339 - accuracy: 0.7238\n",
            "Validation Accuracy: 72.38%\n",
            "Class labels: ['Ant-Man figurine', 'Black Panther figurine', 'Black Widow figurine', 'Captain America figurine', 'Captain Marvel figurine', 'Deku figurine', 'Doctor Strange figurine', 'Eren figurine', 'Goku figurine', 'Hulk figurine', 'Ichigo figurine', 'Iron Man figurine', 'Luffy figurine', 'Naruto figurine', 'Natsu figurine', 'Saitama figurine', 'Sasuke figurine', 'Spider-Man figurine', 'Thor figurine', 'Vegeta figurine']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As this model was overfiiting the training set previously, increasing the training set to 60 images for a character and training for more number of iterations has increased its accuracy from 59% to 73%."
      ],
      "metadata": {
        "id": "_zyVJSMxVYzs"
      }
    }
  ]
}